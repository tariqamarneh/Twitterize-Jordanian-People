{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4156b4a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json  \n",
    "\n",
    "f1=  open(r\"C:\\Users\\tareq\\Desktop\\p2\\Codes\\tweets_status1.json\")\n",
    "f2 = open(r\"C:\\Users\\tareq\\Desktop\\p2\\Codes\\tweets_status2.json\")\n",
    "f3 = open(r\"C:\\Users\\tareq\\Desktop\\p2\\Codes\\tweets_status3.json\")\n",
    "f4 = open(r\"C:\\Users\\tareq\\Desktop\\p2\\Codes\\tweets_status4.json\")\n",
    "f5 = open(r\"C:\\Users\\tareq\\Desktop\\p2\\Codes\\tweets_status5.json\")\n",
    "f6 = open(r\"C:\\Users\\tareq\\Desktop\\p2\\Codes\\tweets_status6.json\")\n",
    "f7 = open(r\"C:\\Users\\tareq\\Desktop\\p2\\Codes\\tweets_status7.json\")\n",
    "f8 = open(r\"C:\\Users\\tareq\\Desktop\\p2\\Codes\\tweets_status8.json\")\n",
    "f9 = open(r\"C:\\Users\\tareq\\Desktop\\p2\\Codes\\tweets_status9.json\")\n",
    "\n",
    "\n",
    "uu1 = json.load(f1)\n",
    "uu2 = json.load(f2)\n",
    "uu3 = json.load(f3)\n",
    "uu4 = json.load(f4)\n",
    "uu5 = json.load(f5)\n",
    "uu6 = json.load(f6)\n",
    "uu7 = json.load(f7)\n",
    "uu8 = json.load(f8)\n",
    "uu9 = json.load(f9)\n",
    "uu=uu1+uu2+uu3+uu4+uu5+uu6+uu7+uu8+uu9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d133357",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=[]\n",
    "for i in uu:\n",
    "    username=i[0]\n",
    "    for j in i[1]:\n",
    "        full_text=j['full_text']\n",
    "        created_at=j['created_at']\n",
    "        lang=j['lang']\n",
    "        data.append([username,full_text,lang,created_at])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ddbdeda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df=pd.DataFrame(data,columns=['Username','Full_Text','Lang','Created_At'])\n",
    "df.to_csv('Username_Fulltext.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb81011",
   "metadata": {},
   "outputs": [],
   "source": [
    "#zxx url\n",
    "#qme url and emojes\n",
    "#und arabic and english\n",
    "#art emojies\n",
    "#tl arabic in englis\n",
    "#in arabic in english\n",
    "#qht hashtags\n",
    "#qst url with random text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9051fd79",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df[(df['Lang']=='ar') | (df['Lang']=='en') | (df['Lang']=='und') | (df['Lang']=='art') | (df['Lang']=='qht')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bbc22a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import emoji\n",
    "def avg_word(sentence):\n",
    "    words = sentence.split()\n",
    "    if len(words) == 0:\n",
    "        return 0\n",
    "    return (sum(len(word) for word in words)/len(words))\n",
    "\n",
    "def emoji_counter(sentence):\n",
    "    return emoji.emoji_count(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bbdd3188",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['Word_Coun']=df['Full_Text'].apply(lambda x: len(str(x).split(\" \")))\n",
    "df['Char_Count'] = df['Full_Text'].str.len()\n",
    "df['Avg_Char_Per_Word'] = df['Full_Text'].apply(lambda x: avg_word(x))\n",
    "df['Emoji_Count'] = df['Full_Text'].apply(lambda x: emoji_counter(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b471c11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\tareq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Text standarisation\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from textblob import TextBlob\n",
    "import re\n",
    "from tashaphyne.stemming import ArabicLightStemmer\n",
    "from nltk.stem.isri import ISRIStemmer\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "stops = set(stopwords.words(\"arabic\"))\n",
    "stop_word_comp = {\"،\",\"آض\",\"آمينَ\",\"آه\",\"آهاً\",\"آي\",\"أ\",\"أب\",\"أجل\",\"أجمع\",\"أخ\",\"أخذ\",\"أصبح\",\"أضحى\",\"أقبل\",\"أقل\",\"أكثر\",\"ألا\",\"أم\",\"أما\",\"أمامك\",\"أمامكَ\",\"أمسى\",\"أمّا\",\"أن\",\"أنا\",\"أنت\",\"أنتم\",\"أنتما\",\"أنتن\",\"أنتِ\",\"أنشأ\",\"أنّى\",\"أو\",\"أوشك\",\"أولئك\",\"أولئكم\",\"أولاء\",\"أولالك\",\"أوّهْ\",\"أي\",\"أيا\",\"أين\",\"أينما\",\"أيّ\",\"أَنَّ\",\"أََيُّ\",\"أُفٍّ\",\"إذ\",\"إذا\",\"إذاً\",\"إذما\",\"إذن\",\"إلى\",\"إليكم\",\"إليكما\",\"إليكنّ\",\"إليكَ\",\"إلَيْكَ\",\"إلّا\",\"إمّا\",\"إن\",\"إنّما\",\"إي\",\"إياك\",\"إياكم\",\"إياكما\",\"إياكن\",\"إيانا\",\"إياه\",\"إياها\",\"إياهم\",\"إياهما\",\"إياهن\",\"إياي\",\"إيهٍ\",\"إِنَّ\",\"ا\",\"ابتدأ\",\"اثر\",\"اجل\",\"احد\",\"اخرى\",\"اخلولق\",\"اذا\",\"اربعة\",\"ارتدّ\",\"استحال\",\"اطار\",\"اعادة\",\"اعلنت\",\"اف\",\"اكثر\",\"اكد\",\"الألاء\",\"الألى\",\"الا\",\"الاخيرة\",\"الان\",\"الاول\",\"الاولى\",\"التى\",\"التي\",\"الثاني\",\"الثانية\",\"الذاتي\",\"الذى\",\"الذي\",\"الذين\",\"السابق\",\"الف\",\"اللائي\",\"اللاتي\",\"اللتان\",\"اللتيا\",\"اللتين\",\"اللذان\",\"اللذين\",\"اللواتي\",\"الماضي\",\"المقبل\",\"الوقت\",\"الى\",\"اليوم\",\"اما\",\"امام\",\"امس\",\"ان\",\"انبرى\",\"انقلب\",\"انه\",\"انها\",\"او\",\"اول\",\"اي\",\"ايار\",\"ايام\",\"ايضا\",\"ب\",\"بات\",\"باسم\",\"بان\",\"بخٍ\",\"برس\",\"بسبب\",\"بسّ\",\"بشكل\",\"بضع\",\"بطآن\",\"بعد\",\"بعض\",\"بك\",\"بكم\",\"بكما\",\"بكن\",\"بل\",\"بلى\",\"بما\",\"بماذا\",\"بمن\",\"بن\",\"بنا\",\"به\",\"بها\",\"بي\",\"بيد\",\"بين\",\"بَسْ\",\"بَلْهَ\",\"بِئْسَ\",\"تانِ\",\"تانِك\",\"تبدّل\",\"تجاه\",\"تحوّل\",\"تلقاء\",\"تلك\",\"تلكم\",\"تلكما\",\"تم\",\"تينك\",\"تَيْنِ\",\"تِه\",\"تِي\",\"ثلاثة\",\"ثم\",\"ثمّ\",\"ثمّة\",\"ثُمَّ\",\"جعل\",\"جلل\",\"جميع\",\"جير\",\"حار\",\"حاشا\",\"حاليا\",\"حاي\",\"حتى\",\"حرى\",\"حسب\",\"حم\",\"حوالى\",\"حول\",\"حيث\",\"حيثما\",\"حين\",\"حيَّ\",\"حَبَّذَا\",\"حَتَّى\",\"حَذارِ\",\"خلا\",\"خلال\",\"دون\",\"دونك\",\"ذا\",\"ذات\",\"ذاك\",\"ذانك\",\"ذانِ\",\"ذلك\",\"ذلكم\",\"ذلكما\",\"ذلكن\",\"ذو\",\"ذوا\",\"ذواتا\",\"ذواتي\",\"ذيت\",\"ذينك\",\"ذَيْنِ\",\"ذِه\",\"ذِي\",\"راح\",\"رجع\",\"رويدك\",\"ريث\",\"رُبَّ\",\"زيارة\",\"سبحان\",\"سرعان\",\"سنة\",\"سنوات\",\"سوف\",\"سوى\",\"سَاءَ\",\"سَاءَمَا\",\"شبه\",\"شخصا\",\"شرع\",\"شَتَّانَ\",\"صار\",\"صباح\",\"صفر\",\"صهٍ\",\"صهْ\",\"ضد\",\"ضمن\",\"طاق\",\"طالما\",\"طفق\",\"طَق\",\"ظلّ\",\"عاد\",\"عام\",\"عاما\",\"عامة\",\"عدا\",\"عدة\",\"عدد\",\"عدم\",\"عسى\",\"عشر\",\"عشرة\",\"علق\",\"على\",\"عليك\",\"عليه\",\"عليها\",\"علًّ\",\"عن\",\"عند\",\"عندما\",\"عوض\",\"عين\",\"عَدَسْ\",\"عَمَّا\",\"غدا\",\"غير\",\"ـ\",\"ف\",\"فان\",\"فلان\",\"فو\",\"فى\",\"في\",\"فيم\",\"فيما\",\"فيه\",\"فيها\",\"قال\",\"قام\",\"قبل\",\"قد\",\"قطّ\",\"قلما\",\"قوة\",\"كأنّما\",\"كأين\",\"كأيّ\",\"كأيّن\",\"كاد\",\"كان\",\"كانت\",\"كذا\",\"كذلك\",\"كرب\",\"كل\",\"كلا\",\"كلاهما\",\"كلتا\",\"كلم\",\"كليكما\",\"كليهما\",\"كلّما\",\"كلَّا\",\"كم\",\"كما\",\"كي\",\"كيت\",\"كيف\",\"كيفما\",\"كَأَنَّ\",\"كِخ\",\"لئن\",\"لا\",\"لات\",\"لاسيما\",\"لدن\",\"لدى\",\"لعمر\",\"لقاء\",\"لك\",\"لكم\",\"لكما\",\"لكن\",\"لكنَّما\",\"لكي\",\"لكيلا\",\"للامم\",\"لم\",\"لما\",\"لمّا\",\"لن\",\"لنا\",\"له\",\"لها\",\"لو\",\"لوكالة\",\"لولا\",\"لوما\",\"لي\",\"لَسْتَ\",\"لَسْتُ\",\"لَسْتُم\",\"لَسْتُمَا\",\"لَسْتُنَّ\",\"لَسْتِ\",\"لَسْنَ\",\"لَعَلَّ\",\"لَكِنَّ\",\"لَيْتَ\",\"لَيْسَ\",\"لَيْسَا\",\"لَيْسَتَا\",\"لَيْسَتْ\",\"لَيْسُوا\",\"لَِسْنَا\",\"ما\",\"ماانفك\",\"مابرح\",\"مادام\",\"ماذا\",\"مازال\",\"مافتئ\",\"مايو\",\"متى\",\"مثل\",\"مذ\",\"مساء\",\"مع\",\"معاذ\",\"مقابل\",\"مكانكم\",\"مكانكما\",\"مكانكنّ\",\"مكانَك\",\"مليار\",\"مليون\",\"مما\",\"ممن\",\"من\",\"منذ\",\"منها\",\"مه\",\"مهما\",\"مَنْ\",\"مِن\",\"نحن\",\"نحو\",\"نعم\",\"نفس\",\"نفسه\",\"نهاية\",\"نَخْ\",\"نِعِمّا\",\"نِعْمَ\",\"ها\",\"هاؤم\",\"هاكَ\",\"هاهنا\",\"هبّ\",\"هذا\",\"هذه\",\"هكذا\",\"هل\",\"هلمَّ\",\"هلّا\",\"هم\",\"هما\",\"هن\",\"هنا\",\"هناك\",\"هنالك\",\"هو\",\"هي\",\"هيا\",\"هيت\",\"هيّا\",\"هَؤلاء\",\"هَاتانِ\",\"هَاتَيْنِ\",\"هَاتِه\",\"هَاتِي\",\"هَجْ\",\"هَذا\",\"هَذانِ\",\"هَذَيْنِ\",\"هَذِه\",\"هَذِي\",\"هَيْهَاتَ\",\"و\",\"و6\",\"وا\",\"واحد\",\"واضاف\",\"واضافت\",\"واكد\",\"وان\",\"واهاً\",\"واوضح\",\"وراءَك\",\"وفي\",\"وقال\",\"وقالت\",\"وقد\",\"وقف\",\"وكان\",\"وكانت\",\"ولا\",\"ولم\",\"ومن\",\"مَن\",\"وهو\",\"وهي\",\"ويكأنّ\",\"وَيْ\",\"وُشْكَانََ\",\"يكون\",\"يمكن\",\"يوم\",\"ّأيّان\"}\n",
    "ArListem = ArabicLightStemmer()\n",
    "\n",
    "\n",
    "import mtranslate\n",
    "from textblob import TextBlob\n",
    "def to_arabic(text):\n",
    "    text=text.split(' ')\n",
    "    clean= []\n",
    "    for i in text:\n",
    "        if re.match(r'^[a-zA-Z]+$', i):\n",
    "            translated_text = mtranslate.translate(i, \"ar\")\n",
    "            clean.append(translated_text)\n",
    "        else:\n",
    "            clean.append(i)\n",
    "    return \" \".join(clean)\n",
    "\n",
    "def stem(text):\n",
    "    zen = TextBlob(text)\n",
    "    words = zen.words\n",
    "    cleaned = list()\n",
    "    for w in words:\n",
    "        ArListem.light_stem(w)\n",
    "        cleaned.append(ArListem.get_root())\n",
    "    return \" \".join(cleaned)\n",
    "import pyarabic.araby as araby\n",
    "def normalizeArabic(text):\n",
    "    text = text.strip()\n",
    "    text = re.sub(\"[إأٱآا]\", \"ا\", text)\n",
    "    text = re.sub(\"ى\", \"ي\", text)\n",
    "    text = re.sub(\"ؤ\", \"ء\", text)\n",
    "    text = re.sub(\"ئ\", \"ء\", text)\n",
    "    text = re.sub(\"ة\", \"ه\", text)\n",
    "    noise = re.compile(\"\"\" ّ    | # Tashdid\n",
    "                             َ    | # Fatha\n",
    "                             ً    | # Tanwin Fath\n",
    "                             ُ    | # Damma\n",
    "                             ٌ    | # Tanwin Damm\n",
    "                             ِ    | # Kasra\n",
    "                             ٍ    | # Tanwin Kasr\n",
    "                             ْ    | # Sukun\n",
    "                             ـ     # Tatwil/Kashida\n",
    "                         \"\"\", re.VERBOSE)\n",
    "    text = re.sub(noise, '', text)\n",
    "    text = re.sub(r'(.)\\1+', r\"\\1\\1\", text) # Remove longation\n",
    "    return araby.strip_tashkeel(text)\n",
    "\n",
    "def remove_stop_words(text):\n",
    "    zen = TextBlob(text)\n",
    "    words = zen.words\n",
    "    return \" \".join([w for w in words if not w in stops and not w in stop_word_comp and len(w) >= 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b5f4d900",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dealing with hashtag\n",
    "def split_hashtag_to_words(tag):\n",
    "    tag = tag.replace('#','')\n",
    "    tags = tag.split('_')\n",
    "    if len(tags) > 1 :\n",
    "        return tags\n",
    "    pattern = re.compile(r\"[A-Z][a-z]+|\\d+|[A-Z]+(?![a-z])\")\n",
    "    return pattern.findall(tag)\n",
    "\n",
    "def clean_hashtag(text):\n",
    "    words = text.split()\n",
    "    text = list()\n",
    "    for word in words:\n",
    "        if is_hashtag(word):\n",
    "            text.extend(extract_hashtag(word))\n",
    "        else:\n",
    "            text.append(word)\n",
    "    return \" \".join(text)\n",
    "def is_hashtag(word):\n",
    "    if word.startswith(\"#\"):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "def extract_hashtag(text):\n",
    "    \n",
    "    hash_list = ([re.sub(r\"(\\W+)$\", \"\", i) for i in text.split() if i.startswith(\"#\")])\n",
    "    word_list = []\n",
    "    for word in hash_list :\n",
    "        word_list.extend(split_hashtag_to_words(word))\n",
    "    return word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a2f9771",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dealing with emojis\n",
    "with open(r\"C:\\Users\\tareq\\Desktop\\p2\\final_website\\emojis.csv\",'r',encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "    emojis_ar = {}\n",
    "    for line in lines:\n",
    "        line = line.strip('\\n').split(';')\n",
    "        emojis_ar.update({line[0].strip():line[1].strip()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4e24007a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals\n",
    "\n",
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                                   u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                                   u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                                   u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                                   u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                                   u\"\\U00002702-\\U000027B0\"\n",
    "                                   u\"\\U000024C2-\\U0001F251\"\n",
    "                                   \"]+\", flags=re.UNICODE)\n",
    "    text = emoji_pattern.sub(r'', text)\n",
    "    return text\n",
    "\n",
    "import unicodedata\n",
    "from unidecode import unidecode\n",
    "def emoji_native_translation(text):\n",
    "    text = text.lower()\n",
    "    loves = [\"<3\", \"♥\",'❤']\n",
    "    smilefaces = []\n",
    "    sadfaces = []\n",
    "    neutralfaces = []\n",
    "\n",
    "    eyes = [\"8\",\":\",\"=\",\";\"]\n",
    "    nose = [\"'\",\"`\",\"-\",\"\\\\\"]\n",
    "    for e in eyes:\n",
    "        for n in nose:\n",
    "            for s in [\")\", \"d\", \"]\", \"}\",\"p\"]:\n",
    "                smilefaces.append(e+n+s)\n",
    "                smilefaces.append(e+s)\n",
    "            for s in [\"(\", \"[\", \"{\"]:\n",
    "                sadfaces.append(e+n+s)\n",
    "                sadfaces.append(e+s)\n",
    "            for s in [\"|\", \"/\", \"\\\\\"]:\n",
    "                neutralfaces.append(e+n+s)\n",
    "                neutralfaces.append(e+s)\n",
    "            #reversed\n",
    "            for s in [\"(\", \"[\", \"{\"]:\n",
    "                smilefaces.append(s+n+e)\n",
    "                smilefaces.append(s+e)\n",
    "            for s in [\")\", \"]\", \"}\"]:\n",
    "                sadfaces.append(s+n+e)\n",
    "                sadfaces.append(s+e)\n",
    "            for s in [\"|\", \"/\", \"\\\\\"]:\n",
    "                neutralfaces.append(s+n+e)\n",
    "                neutralfaces.append(s+e)\n",
    "\n",
    "    smilefaces = list(set(smilefaces))\n",
    "    sadfaces = list(set(sadfaces))\n",
    "    neutralfaces = list(set(neutralfaces))\n",
    "    t = []\n",
    "    for w in text.split():\n",
    "        if w in loves:\n",
    "            t.append(\"حب\")\n",
    "        elif w in smilefaces:\n",
    "            t.append(\"مضحك\")\n",
    "        elif w in neutralfaces:\n",
    "            t.append(\"عادي\")\n",
    "        elif w in sadfaces:\n",
    "            t.append(\"محزن\")\n",
    "        else:\n",
    "            t.append(w)\n",
    "    newText = \" \".join(t)\n",
    "    return newText\n",
    "import emoji\n",
    "def is_emoji(word):\n",
    "    if word in emojis_ar:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def add_space(text):\n",
    "    return ''.join(' ' + char if is_emoji(char) else char for char in text).strip()\n",
    "\n",
    "from aiogoogletrans import Translator\n",
    "translator = Translator()\n",
    "import asyncio\n",
    "loop = asyncio.get_event_loop()\n",
    "def translate_emojis(words):\n",
    "    word_list = list()\n",
    "    words_to_translate = list()\n",
    "    for word in words :\n",
    "        t = emojis_ar.get(word.get('emoji'),None)\n",
    "        if t is None:\n",
    "            word.update({'translation':'عادي','translated':True})\n",
    "            #words_to_translate.append('normal')\n",
    "        else:\n",
    "            word.update({'translated':False,'translation':t})\n",
    "            words_to_translate.append(t.replace(':','').replace('_',' '))\n",
    "        word_list.append(word)\n",
    "    return word_list\n",
    "def emoji_unicode_translation(text):\n",
    "    text = add_space(text)\n",
    "    words = text.split()\n",
    "    text_list = list()\n",
    "    emojis_list = list()\n",
    "    c = 0\n",
    "    for word in words:\n",
    "        if is_emoji(word):\n",
    "            emojis_list.append({'emoji':word,'emplacement':c})\n",
    "        else:\n",
    "            text_list.append(word)\n",
    "        c+=1\n",
    "    emojis_translated = translate_emojis(emojis_list)\n",
    "    for em in emojis_translated:\n",
    "        text_list.insert(em.get('emplacement'),em.get('translation'))\n",
    "    text = \" \".join(text_list)\n",
    "    return text\n",
    "    \n",
    "def clean_emoji(text):\n",
    "    text = emoji_native_translation(text)\n",
    "    text = emoji_unicode_translation(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f6489c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweet(text):\n",
    "    text = re.sub('#\\d+K\\d+', ' ', text)  # years like 2K19\n",
    "    text = re.sub('http\\S+\\s*', ' ', text)  # remove URLs\n",
    "    text = re.sub('RT|cc', ' ', text)  # remove RT and cc\n",
    "    text = re.sub('@[^\\s]+',' ',text) # remove mintions\n",
    "    text = clean_hashtag(text)\n",
    "    text = clean_emoji(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9bae00dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    ## Clean for tweets\n",
    "    text = clean_tweet(text)\n",
    "    ## Remove punctuations\n",
    "    text = re.sub('[%s]' % re.escape(\"\"\"!\"#$%&'()*+,،-./:;<=>؟?@[\\]^_`{|}~\"\"\"), ' ', text)  # remove punctuation\n",
    "    ## remove extra whitespace\n",
    "    text = re.sub('\\s+', ' ', text)  \n",
    "    ## Remove Emojis\n",
    "    text = remove_emoji(text)\n",
    "    ## Convert text to lowercases\n",
    "    text = text.lower()\n",
    "    ## Arabisy the text\n",
    "    text = to_arabic(text)\n",
    "    ## Remove stop words\n",
    "    text = remove_stop_words(text)\n",
    "    ## Remove numbers\n",
    "    text = re.sub(\"\\d+\", \" \", text)\n",
    "    ## Remove Tashkeel\n",
    "    text = normalizeArabic(text)\n",
    "    #text = re.sub('\\W+', ' ', text)\n",
    "    text = re.sub('[A-Za-z]+',' ',text)\n",
    "    text = re.sub(r'\\\\u[A-Za-z0-9\\\\]+',' ',text)\n",
    "    ## remove extra whitespace\n",
    "    text = re.sub('\\s+', ' ', text)  \n",
    "    #Stemming\n",
    "    #text = stem(text)\n",
    "    return text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1728a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "pool = mp.Pool(mp.cpu_count())\n",
    "\n",
    "# Apply the function to the Full_Text column in parallel\n",
    "df['Full_Text'] = pool.map(clean_text, df['Full_Text'])\n",
    "\n",
    "# Close the multiprocessing pool\n",
    "pool.close()\n",
    "pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5904f93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"Cleaned_Username_Fulltext.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9229a64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddda4e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(r\"C:\\Users\\tareq\\Desktop\\Cleaned_Username_Fulltext.csv\")\n",
    "labeld = pd.read_excel(r\"C:\\Users\\tareq\\Desktop\\p2\\Data\\Labeled.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f73af34",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = df.groupby(\"Username\")\n",
    "aggregated = grouped.agg({\n",
    "    \"Full_Text\": \" \".join,\n",
    "    \"Word_Coun\": \"sum\",\n",
    "    \"Char_Count\": \"sum\",\n",
    "    \"Avg_Char_Per_Word\": \"mean\",\n",
    "    \"Emoji_Count\": \"sum\"\n",
    "})\n",
    "aggregated.columns = [\"Full_Text\", \"total_words\", \"total_char\", \"avg_chr_per_word\", \"total_emojis\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5d4489",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.merge(labeld, aggregated, on='Username', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d995cfb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17533f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032c8919",
   "metadata": {},
   "outputs": [],
   "source": [
    "o=[]\n",
    "c=[]\n",
    "e=[]\n",
    "a=[]\n",
    "n=[]\n",
    "for i in range(len(result)): \n",
    "    o.append(int(result['Label'][i].split()[0][1]))\n",
    "    c.append(int(result['Label'][i].split()[1][0]))\n",
    "    e.append(int(result['Label'][i].split()[2][0]))\n",
    "    a.append(int(result['Label'][i].split()[3][0]))\n",
    "    n.append(int(result['Label'][i].split()[4][0]))\n",
    "result['o']=o\n",
    "result['c']=c\n",
    "result['e']=e\n",
    "result['a']=a\n",
    "result['n']=n\n",
    "result.drop('Label',axis=1,inplace=True)\n",
    "df=result[['Full_Text',\"total_words\", \"total_char\", \"avg_chr_per_word\", \"total_emojis\",'o','c','e','a','n']]\n",
    "df.to_csv(\"All_Users_Labeled.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
