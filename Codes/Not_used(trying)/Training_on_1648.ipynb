{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7544c43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from imblearn.over_sampling import SMOTE,RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec7957fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(r\"C:\\Users\\tareq\\Desktop\\p2\\Codes\\All_Users_Labeled.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0491eee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('Unnamed: 0',axis=1,inplace=True)\n",
    "df.drop('Username',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2535ae19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Full_Text</th>\n",
       "      <th>total_words</th>\n",
       "      <th>total_char</th>\n",
       "      <th>avg_chr_per_word</th>\n",
       "      <th>total_emojis</th>\n",
       "      <th>o</th>\n",
       "      <th>c</th>\n",
       "      <th>e</th>\n",
       "      <th>a</th>\n",
       "      <th>n</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>كلمه انثي صارت تضرب عصبي بمناسبه المراه العالم...</td>\n",
       "      <td>2530.0</td>\n",
       "      <td>17071.0</td>\n",
       "      <td>6.087708</td>\n",
       "      <td>151.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>فش موظفين القهوه الكيوت اللي بحكيلك بنفع يحط ش...</td>\n",
       "      <td>1394.0</td>\n",
       "      <td>8593.0</td>\n",
       "      <td>5.656274</td>\n",
       "      <td>28.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>مباشر القداس الالهي وصلاه المراثي كنيسه القديس...</td>\n",
       "      <td>5404.0</td>\n",
       "      <td>38315.0</td>\n",
       "      <td>6.158481</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ارتفع قهوه ارتفع قلبين قطعه شوكولاته ارتفع قل...</td>\n",
       "      <td>674.0</td>\n",
       "      <td>4665.0</td>\n",
       "      <td>6.584828</td>\n",
       "      <td>48.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>يتدحرج الارض الضحك  فن   غيم غيم   وجه متعب  ...</td>\n",
       "      <td>328.0</td>\n",
       "      <td>2398.0</td>\n",
       "      <td>6.471875</td>\n",
       "      <td>47.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1643</th>\n",
       "      <td>اللهم عمره اعيد شتات قلبي الكعبه يقاس رقي الان...</td>\n",
       "      <td>2436.0</td>\n",
       "      <td>14009.0</td>\n",
       "      <td>4.763297</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1644</th>\n",
       "      <td>تفعيل مهارات الرسم الهندسي لترسيخ مفهوم التماث...</td>\n",
       "      <td>3887.0</td>\n",
       "      <td>26702.0</td>\n",
       "      <td>5.747843</td>\n",
       "      <td>248.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645</th>\n",
       "      <td>تنهد  نار نار  يتدحرج الارض الضحك  انتصار وجه...</td>\n",
       "      <td>646.0</td>\n",
       "      <td>3501.0</td>\n",
       "      <td>4.825641</td>\n",
       "      <td>75.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1646</th>\n",
       "      <td>البعض يبقي اجلك يبقي حاجته  وجه بالدوار  وجه ا...</td>\n",
       "      <td>26.0</td>\n",
       "      <td>136.0</td>\n",
       "      <td>4.161111</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1647</th>\n",
       "      <td>فقدت مهاره النوم متامل جماعه الستوريات انستقرا...</td>\n",
       "      <td>1007.0</td>\n",
       "      <td>5993.0</td>\n",
       "      <td>5.277262</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1648 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Full_Text  total_words  \\\n",
       "0     كلمه انثي صارت تضرب عصبي بمناسبه المراه العالم...       2530.0   \n",
       "1     فش موظفين القهوه الكيوت اللي بحكيلك بنفع يحط ش...       1394.0   \n",
       "2     مباشر القداس الالهي وصلاه المراثي كنيسه القديس...       5404.0   \n",
       "3      ارتفع قهوه ارتفع قلبين قطعه شوكولاته ارتفع قل...        674.0   \n",
       "4      يتدحرج الارض الضحك  فن   غيم غيم   وجه متعب  ...        328.0   \n",
       "...                                                 ...          ...   \n",
       "1643  اللهم عمره اعيد شتات قلبي الكعبه يقاس رقي الان...       2436.0   \n",
       "1644  تفعيل مهارات الرسم الهندسي لترسيخ مفهوم التماث...       3887.0   \n",
       "1645   تنهد  نار نار  يتدحرج الارض الضحك  انتصار وجه...        646.0   \n",
       "1646  البعض يبقي اجلك يبقي حاجته  وجه بالدوار  وجه ا...         26.0   \n",
       "1647  فقدت مهاره النوم متامل جماعه الستوريات انستقرا...       1007.0   \n",
       "\n",
       "      total_char  avg_chr_per_word  total_emojis  o  c  e  a  n  \n",
       "0        17071.0          6.087708         151.0  0  0  1  1  0  \n",
       "1         8593.0          5.656274          28.0  1  0  0  0  0  \n",
       "2        38315.0          6.158481           2.0  0  1  0  1  0  \n",
       "3         4665.0          6.584828          48.0  1  0  0  0  0  \n",
       "4         2398.0          6.471875          47.0  1  0  1  0  0  \n",
       "...          ...               ...           ... .. .. .. .. ..  \n",
       "1643     14009.0          4.763297          11.0  0  0  0  1  0  \n",
       "1644     26702.0          5.747843         248.0  0  0  0  1  0  \n",
       "1645      3501.0          4.825641          75.0  1  0  0  0  0  \n",
       "1646       136.0          4.161111           2.0  1  0  0  0  0  \n",
       "1647      5993.0          5.277262           6.0  0  1  0  0  0  \n",
       "\n",
       "[1648 rows x 10 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bbd99c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_num = df.iloc[:, 1:5].values\n",
    "ss=StandardScaler()\n",
    "X_num=ss.fit_transform(X=X_num)\n",
    "vectorizer = CountVectorizer()\n",
    "X_text = vectorizer.fit_transform(df['Full_Text'])\n",
    "df1=pd.DataFrame(X_text.toarray(),columns=vectorizer.get_feature_names_out())\n",
    "df1['total_words']= [x[0] for x in X_num]\n",
    "df1[ \"total_char\"]= [x[1] for x in X_num]\n",
    "df1[\"avg_chr_per_word\"]= [x[2] for x in X_num]\n",
    "df1[\"total_emojis\"]= [x[3] for x in X_num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01941182",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['o'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d9d51547",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "17/17 [==============================] - 32s 2s/step - loss: 944.7569 - accuracy: 0.9345 - val_loss: 26.7165 - val_accuracy: 0.9470\n",
      "Epoch 2/20\n",
      "17/17 [==============================] - 30s 2s/step - loss: 194.6606 - accuracy: 0.6262 - val_loss: 138.8735 - val_accuracy: 0.0530\n",
      "Epoch 3/20\n",
      "17/17 [==============================] - 30s 2s/step - loss: 60.3844 - accuracy: 0.5930 - val_loss: 11.9525 - val_accuracy: 0.3636\n",
      "Epoch 4/20\n",
      "17/17 [==============================] - 36s 2s/step - loss: 27.8611 - accuracy: 0.4241 - val_loss: 2.1007 - val_accuracy: 0.7235\n",
      "Epoch 5/20\n",
      "17/17 [==============================] - 42s 2s/step - loss: 19.8157 - accuracy: 0.5825 - val_loss: 10.4312 - val_accuracy: 0.3371\n",
      "Epoch 6/20\n",
      "17/17 [==============================] - 32s 2s/step - loss: 19.1174 - accuracy: 0.5446 - val_loss: 11.0839 - val_accuracy: 0.2765\n",
      "Epoch 7/20\n",
      "17/17 [==============================] - 31s 2s/step - loss: 14.4393 - accuracy: 0.5474 - val_loss: 2.4525 - val_accuracy: 0.6098\n",
      "Epoch 7: early stopping\n",
      "11/11 [==============================] - 2s 184ms/step - loss: 3.7171 - accuracy: 0.5333\n",
      "Test Loss: 3.717148542404175, Test Accuracy: 0.5333333611488342\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Dense, Embedding, LSTM, Input, Concatenate,Dropout,Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "\n",
    "X = df['Full_Text']\n",
    "#y = df[['o', 'c', 'e', 'a', 'n']]\n",
    "y=df['n']\n",
    "other_features = df[['total_words', 'total_char', 'avg_chr_per_word', 'total_emojis']]\n",
    "\n",
    "X_train, X_test, y_train, y_test, other_features_train, other_features_test = train_test_split(X, y, other_features, test_size=0.2, random_state=42)\n",
    "\n",
    "# Tokenize the input texts\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=95000, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# Pad the input sequences\n",
    "max_len = 2000 # Set a max length that captures most of the context without introducing too much padding\n",
    "X_train_padded = pad_sequences(X_train_seq, maxlen=max_len, padding='post', truncating='post')\n",
    "X_test_padded = pad_sequences(X_test_seq, maxlen=max_len, padding='post', truncating='post')\n",
    "\n",
    "# Convert labels to categorical format\n",
    "#y_train_cat = to_categorical(y_train)\n",
    "#y_test_cat = to_categorical(y_test)\n",
    "\n",
    "# Define the other features input layer\n",
    "other_features_input = Input(shape=(4,), name='other_features_input')\n",
    "\n",
    "# Define the text input layer\n",
    "text_input = Input(shape=(max_len,), name='text_input')\n",
    "\n",
    "# Define the embedding layer for the text input\n",
    "embedding_dim = 128\n",
    "embedded_text = Embedding(input_dim=175277, output_dim=embedding_dim, input_length=max_len)(text_input)\n",
    "\n",
    "# Define the LSTM layer\n",
    "lstm = LSTM(units=16)(embedded_text)\n",
    "# Add Dense layer to reshape input_features\n",
    "dense = Dense(units=16)(other_features_input)\n",
    "\n",
    "# Concatenate the LSTM output and dense layer output\n",
    "concatenated = Concatenate()([lstm, dense])\n",
    "\n",
    "# Define the output layer\n",
    "\n",
    "output = Dense(units=1, activation='sigmoid')(concatenated)\n",
    "\n",
    "# Define the model with both the text input and other features input\n",
    "model = Model(inputs=[text_input, other_features_input], outputs=output)\n",
    "\n",
    "# compute class weights\n",
    "total = y.shape[0]\n",
    "class_0 = np.sum(y == 0)\n",
    "class_1 = np.sum(y == 1)\n",
    "w0 = total / class_0\n",
    "w1 = total / class_1\n",
    "class_weight = {0: w0, 1: w1}\n",
    "\n",
    "# Compile the model\n",
    "from keras.optimizers import SGD\n",
    "opt = SGD(learning_rate=10, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss = \"binary_crossentropy\", optimizer = 'adam',metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "es = EarlyStopping(patience=3, verbose=1)\n",
    "history = model.fit(\n",
    "    x=[X_train_padded, other_features_train],\n",
    "    y=y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=20,\n",
    "    batch_size=64,\n",
    "    #shuffle=True,\n",
    "    class_weight=class_weight,\n",
    "    callbacks=[es]\n",
    ")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_acc = model.evaluate(x=[X_test_padded, other_features_test], y=y_test)\n",
    "print(f'Test Loss: {test_loss}, Test Accuracy: {test_acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796b1ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=model.predict(x=[X_test_padded, other_features_test])\n",
    "y_pred=y_pred>0.3\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b4141d3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla, lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Final_models/rnn_o\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Final_models/rnn_o\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save('Final_models/rnn_o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "3bc76b45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "17/17 [==============================] - 37s 2s/step - loss: 1493.1764 - accuracy: 0.4317 - val_loss: 1085.8458 - val_accuracy: 0.4091\n",
      "Epoch 2/20\n",
      "17/17 [==============================] - 34s 2s/step - loss: 616.7061 - accuracy: 0.4326 - val_loss: 169.4843 - val_accuracy: 0.4129\n",
      "Epoch 3/20\n",
      "17/17 [==============================] - 33s 2s/step - loss: 96.5139 - accuracy: 0.5465 - val_loss: 71.0002 - val_accuracy: 0.5909\n",
      "Epoch 4/20\n",
      "17/17 [==============================] - 34s 2s/step - loss: 34.3686 - accuracy: 0.5370 - val_loss: 27.5038 - val_accuracy: 0.5909\n",
      "Epoch 5/20\n",
      "17/17 [==============================] - 34s 2s/step - loss: 18.1529 - accuracy: 0.5550 - val_loss: 15.1530 - val_accuracy: 0.4811\n",
      "Epoch 6/20\n",
      "17/17 [==============================] - 31s 2s/step - loss: 15.0250 - accuracy: 0.5342 - val_loss: 13.7101 - val_accuracy: 0.4773\n",
      "Epoch 7/20\n",
      "17/17 [==============================] - 32s 2s/step - loss: 12.4025 - accuracy: 0.5171 - val_loss: 12.3087 - val_accuracy: 0.4811\n",
      "Epoch 8/20\n",
      "17/17 [==============================] - 31s 2s/step - loss: 11.1267 - accuracy: 0.5218 - val_loss: 11.1426 - val_accuracy: 0.4735\n",
      "Epoch 9/20\n",
      "17/17 [==============================] - 31s 2s/step - loss: 9.3416 - accuracy: 0.5114 - val_loss: 11.3679 - val_accuracy: 0.4773\n",
      "Epoch 10/20\n",
      "17/17 [==============================] - 31s 2s/step - loss: 8.3534 - accuracy: 0.5114 - val_loss: 7.4961 - val_accuracy: 0.5038\n",
      "Epoch 11/20\n",
      "17/17 [==============================] - 32s 2s/step - loss: 7.1632 - accuracy: 0.5057 - val_loss: 5.5527 - val_accuracy: 0.3977\n",
      "Epoch 12/20\n",
      "17/17 [==============================] - 31s 2s/step - loss: 4.9037 - accuracy: 0.4715 - val_loss: 5.2155 - val_accuracy: 0.5038\n",
      "Epoch 13/20\n",
      "17/17 [==============================] - 31s 2s/step - loss: 5.1684 - accuracy: 0.4943 - val_loss: 3.0541 - val_accuracy: 0.4015\n",
      "Epoch 14/20\n",
      "17/17 [==============================] - 32s 2s/step - loss: 3.6150 - accuracy: 0.4459 - val_loss: 2.1305 - val_accuracy: 0.4432\n",
      "Epoch 15/20\n",
      "17/17 [==============================] - 39s 2s/step - loss: 7.3283 - accuracy: 0.4915 - val_loss: 2.9945 - val_accuracy: 0.4735\n",
      "Epoch 16/20\n",
      "17/17 [==============================] - 43s 2s/step - loss: 6.7767 - accuracy: 0.5057 - val_loss: 7.9923 - val_accuracy: 0.4091\n",
      "Epoch 17/20\n",
      "17/17 [==============================] - 35s 2s/step - loss: 5.7493 - accuracy: 0.4592 - val_loss: 2.0923 - val_accuracy: 0.4432\n",
      "Epoch 18/20\n",
      "17/17 [==============================] - 34s 2s/step - loss: 4.3789 - accuracy: 0.4858 - val_loss: 8.5903 - val_accuracy: 0.5227\n",
      "Epoch 19/20\n",
      "17/17 [==============================] - 37s 2s/step - loss: 5.9651 - accuracy: 0.4706 - val_loss: 1.7473 - val_accuracy: 0.4886\n",
      "Epoch 20/20\n",
      "17/17 [==============================] - 35s 2s/step - loss: 2.3988 - accuracy: 0.4924 - val_loss: 4.2457 - val_accuracy: 0.5114\n",
      "11/11 [==============================] - 2s 200ms/step - loss: 3.8574 - accuracy: 0.5636\n",
      "Test Loss: 3.8574366569519043, Test Accuracy: 0.5636363625526428\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Dense, Embedding, LSTM, Input, Concatenate,Dropout,Flatten,\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "\n",
    "X = df['Full_Text']\n",
    "#y = df[['o', 'c', 'e', 'a', 'n']]\n",
    "y=df['c']\n",
    "other_features = df[['total_words', 'total_char', 'avg_chr_per_word', 'total_emojis']]\n",
    "\n",
    "X_train, X_test, y_train, y_test, other_features_train, other_features_test = train_test_split(X, y, other_features, test_size=0.2, random_state=42)\n",
    "\n",
    "# Tokenize the input texts\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=95000, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# Pad the input sequences\n",
    "max_len = 2000 # Set a max length that captures most of the context without introducing too much padding\n",
    "X_train_padded = pad_sequences(X_train_seq, maxlen=max_len, padding='post', truncating='post')\n",
    "X_test_padded = pad_sequences(X_test_seq, maxlen=max_len, padding='post', truncating='post')\n",
    "\n",
    "# Convert labels to categorical format\n",
    "#y_train_cat = to_categorical(y_train)\n",
    "#y_test_cat = to_categorical(y_test)\n",
    "\n",
    "# Define the other features input layer\n",
    "other_features_input = Input(shape=(4,), name='other_features_input')\n",
    "\n",
    "# Define the text input layer\n",
    "text_input = Input(shape=(max_len,), name='text_input')\n",
    "\n",
    "# Define the embedding layer for the text input\n",
    "embedding_dim = 128\n",
    "embedded_text = Embedding(input_dim=175277, output_dim=embedding_dim, input_length=max_len)(text_input)\n",
    "\n",
    "# Define the LSTM layer\n",
    "lstm = LSTM(units=8)(embedded_text)\n",
    "# Add Dense layer 8 reshape input_features\n",
    "dense = Dense(units=16)(other_features_input)\n",
    "\n",
    "# Concatenate the LSTM output and dense layer output\n",
    "concatenated = Concatenate()([lstm, dense])\n",
    "\n",
    "# Define the output layer\n",
    "\n",
    "output = Dense(units=1, activation='sigmoid')(concatenated)\n",
    "\n",
    "# Define the model with both the text input and other features input\n",
    "model = Model(inputs=[text_input, other_features_input], outputs=output)\n",
    "\n",
    "# compute class weights\n",
    "total = y.shape[0]\n",
    "class_0 = np.sum(y == 0)\n",
    "class_1 = np.sum(y == 1)\n",
    "w0 = total / class_0\n",
    "w1 = total / class_1\n",
    "class_weight = {0: w0, 1: w1}\n",
    "\n",
    "# Compile the model\n",
    "from keras.optimizers import SGD\n",
    "opt = SGD(learning_rate=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss = \"binary_crossentropy\", optimizer = 'adam',metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "es = EarlyStopping(patience=3, verbose=1)\n",
    "history = model.fit(\n",
    "    x=[X_train_padded, other_features_train],\n",
    "    y=y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=20,\n",
    "    batch_size=64,\n",
    "    #shuffle=True,\n",
    "    #class_weight=class_weight,\n",
    "    callbacks=[es]\n",
    ")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_acc = model.evaluate(x=[X_test_padded, other_features_test], y=y_test)\n",
    "print(f'Test Loss: {test_loss}, Test Accuracy: {test_acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "014742e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\tareq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "42/42 [==============================] - 15s 273ms/step - loss: 1.9738 - accuracy: 0.5330 - val_loss: 1.6143 - val_accuracy: 0.7394\n",
      "Epoch 2/20\n",
      "42/42 [==============================] - 10s 241ms/step - loss: 1.4589 - accuracy: 0.5816 - val_loss: 1.2742 - val_accuracy: 0.7394\n",
      "Epoch 3/20\n",
      "42/42 [==============================] - 10s 249ms/step - loss: 1.1313 - accuracy: 0.6529 - val_loss: 1.0676 - val_accuracy: 0.7394\n",
      "Epoch 4/20\n",
      "42/42 [==============================] - 10s 245ms/step - loss: 0.9384 - accuracy: 0.6916 - val_loss: 0.9380 - val_accuracy: 0.7394\n",
      "Epoch 5/20\n",
      "42/42 [==============================] - 10s 242ms/step - loss: 0.8131 - accuracy: 0.7174 - val_loss: 0.8489 - val_accuracy: 0.7394\n",
      "Epoch 6/20\n",
      "42/42 [==============================] - 11s 252ms/step - loss: 0.7341 - accuracy: 0.7231 - val_loss: 0.7881 - val_accuracy: 0.7394\n",
      "Epoch 7/20\n",
      "42/42 [==============================] - 10s 246ms/step - loss: 0.6788 - accuracy: 0.7291 - val_loss: 0.7462 - val_accuracy: 0.7394\n",
      "Epoch 8/20\n",
      "42/42 [==============================] - 10s 245ms/step - loss: 0.6463 - accuracy: 0.7489 - val_loss: 0.7138 - val_accuracy: 0.7394\n",
      "Epoch 9/20\n",
      "42/42 [==============================] - 10s 249ms/step - loss: 0.6000 - accuracy: 0.7587 - val_loss: 0.6868 - val_accuracy: 0.7455\n",
      "Epoch 10/20\n",
      "42/42 [==============================] - 10s 243ms/step - loss: 0.5657 - accuracy: 0.7534 - val_loss: 0.6754 - val_accuracy: 0.7515\n",
      "Epoch 11/20\n",
      "42/42 [==============================] - 10s 245ms/step - loss: 0.5378 - accuracy: 0.7618 - val_loss: 0.6743 - val_accuracy: 0.7576\n",
      "Epoch 12/20\n",
      "42/42 [==============================] - 10s 250ms/step - loss: 0.5207 - accuracy: 0.7644 - val_loss: 0.6842 - val_accuracy: 0.7515\n",
      "Epoch 13/20\n",
      "42/42 [==============================] - 10s 245ms/step - loss: 0.5036 - accuracy: 0.7656 - val_loss: 0.6934 - val_accuracy: 0.7515\n",
      "Epoch 14/20\n",
      "42/42 [==============================] - 10s 248ms/step - loss: 0.4976 - accuracy: 0.7618 - val_loss: 0.7492 - val_accuracy: 0.7515\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 0.7916 - accuracy: 0.6909\n",
      "Test Loss: 0.7915742993354797\n",
      "Test Accuracy: 0.6909090876579285\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, GRU, Dense, Dropout, BatchNormalization\n",
    "from keras import regularizers, callbacks\n",
    "import nlpaug.augmenter.word as naw\n",
    "\n",
    "# load dataset\n",
    "data = df.copy()\n",
    "\n",
    "# split dataset into training, validation, and testing sets\n",
    "train_data, temp_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "val_data, test_data = train_test_split(temp_data, test_size=0.5, random_state=42)\n",
    "\n",
    "# data augmentation using nlpaug library\n",
    "aug = naw.SynonymAug(aug_src='wordnet')\n",
    "\n",
    "train_aug_data = train_data.copy()\n",
    "for i in range(10):\n",
    "    train_aug_data['Full_Text'] = train_aug_data['Full_Text'].apply(lambda x: aug.augment(x))\n",
    "train_aug_data = train_aug_data.sample(frac=1)\n",
    "\n",
    "# combine augmented data with original training data\n",
    "train_data = pd.concat([train_data, train_aug_data], ignore_index=True)\n",
    "\n",
    "# tokenize text data\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(train_data['Full_Text'])\n",
    "X_train = tokenizer.texts_to_sequences(train_data['Full_Text'])\n",
    "X_val = tokenizer.texts_to_sequences(val_data['Full_Text'])\n",
    "X_test = tokenizer.texts_to_sequences(test_data['Full_Text'])\n",
    "\n",
    "# pad sequences to make them of the same length\n",
    "maxlen = 100  # max length of a text sequence\n",
    "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
    "X_val = pad_sequences(X_val, padding='post', maxlen=maxlen)\n",
    "X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "ee59c9ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "42/42 [==============================] - 66s 1s/step - loss: 0.8278 - accuracy: 0.6028 - val_loss: 0.7762 - val_accuracy: 0.7394\n",
      "Epoch 2/20\n",
      "42/42 [==============================] - 66s 2s/step - loss: 0.7541 - accuracy: 0.6859 - val_loss: 0.7110 - val_accuracy: 0.7394\n",
      "Epoch 3/20\n",
      "42/42 [==============================] - 65s 2s/step - loss: 0.7059 - accuracy: 0.6984 - val_loss: 0.6653 - val_accuracy: 0.7394\n",
      "Epoch 4/20\n",
      "42/42 [==============================] - 66s 2s/step - loss: 0.6716 - accuracy: 0.6988 - val_loss: 0.6345 - val_accuracy: 0.7394\n",
      "Epoch 5/20\n",
      " 7/42 [====>.........................] - ETA: 56s - loss: 0.6770 - accuracy: 0.6652"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11992\\381232375.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;31m# train the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'o'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'o'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mearly_stopping\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;31m# evaluate the model on test data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1648\u001b[0m                         ):\n\u001b[0;32m   1649\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1650\u001b[1;33m                             \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1651\u001b[0m                             \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1652\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    878\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    879\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 880\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    881\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    882\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    910\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    911\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 912\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_no_variable_creation_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    913\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_variable_creation_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    914\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    132\u001b[0m       (concrete_function,\n\u001b[0;32m    133\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m--> 134\u001b[1;33m     return concrete_function._call_flat(\n\u001b[0m\u001b[0;32m    135\u001b[0m         filtered_flat_args, captured_inputs=concrete_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m    136\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1743\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1744\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1745\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1746\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    376\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    377\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 378\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    379\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    380\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     50\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     53\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     54\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# pad sequences to make them of the same length\n",
    "maxlen = 2000  # max length of a text sequence\n",
    "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
    "X_val = pad_sequences(X_val, padding='post', maxlen=maxlen)\n",
    "X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)\n",
    "\n",
    "# define model architecture\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(tokenizer.word_index)+1, output_dim=100, input_length=maxlen))\n",
    "model.add(GRU(16, kernel_regularizer=regularizers.l2(0.001), recurrent_regularizer=regularizers.l2(0.001)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(8, activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# define early stopping callback\n",
    "early_stopping = callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
    "\n",
    "# train the model\n",
    "model.fit(X_train, train_data['o'], validation_data=(X_val, val_data['o']), epochs=20, batch_size=64, callbacks=[early_stopping])\n",
    "\n",
    "# evaluate the model on test data\n",
    "loss, accuracy = model.evaluate(X_test, test_data['o'])\n",
    "print('Test Loss:', loss)\n",
    "print('Test Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "ef83d09d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 1s 151ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred=model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "6f0b9ef4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([165])"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(y_pred>0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "cbb07171",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "794"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(train_data['o'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6acd127b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7606528c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['c'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "30ae3f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test= train_test_split(df1, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79521392",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "ro=SMOTE()\n",
    "X_train,y_train= ro.fit_resample(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7200fcc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier,RandomForestClassifier\n",
    "r=RandomForestClassifier(random_state=42,class_weight='balanced_subsample')\n",
    "m=BaggingClassifier( random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0cf46e8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaggingClassifier(random_state=42)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "662297f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=m.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9c581fa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.94      0.90       271\n",
      "           1       0.50      0.29      0.37        59\n",
      "\n",
      "    accuracy                           0.82       330\n",
      "   macro avg       0.68      0.61      0.63       330\n",
      "weighted avg       0.79      0.82      0.80       330\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4b9d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.86      0.94      0.90       271\n",
    "           1       0.50      0.29      0.37        59\n",
    "\n",
    "    accuracy                           0.82       330\n",
    "   macro avg       0.68      0.61      0.63       330\n",
    "weighted avg       0.79      0.82      0.80       330\n",
    "\n",
    "\n",
    "BaggingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7195d975",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85ee4fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51cce1b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b561803",
   "metadata": {},
   "outputs": [],
   "source": [
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.90      0.88      0.89       247\n",
    "           1       0.67      0.70      0.68        83\n",
    "\n",
    "    accuracy                           0.84       330\n",
    "   macro avg       0.78      0.79      0.79       330\n",
    "weighted avg       0.84      0.84      0.84       330\n",
    "\n",
    "m=RandomForestClassifier(class_weight='balanced_subsample',criterion='entropy',max_features='sqrt',n_estimators=1000,max_depth=170, random_state=42)\n",
    "o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87846ac0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
